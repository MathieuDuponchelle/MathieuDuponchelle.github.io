fragment_downloaded_cb({"url":"2013-06-08-Fun-with-videomixer.html#looks-fine-explain-it-now-","fragment":"Looks fine, explain it now !\nI'll select the meaningful bits, assuming you know python well enough. If not, this is easily translatable to C,\nor any language that can take advantage of GObject introspection's dynamic bindings.\nFirst, let's look at the main.\nThis convenience function will create a timeline with an audio and a video track for us.\nThis is part of the new API. Thanks to that, GES will only discover the file once, discovering meaning learning\nwhat streams are contained in the media, how long it lasts and other infos. Previously, we would discover\nthe file each time we created an object with it, which was not optimized. request_sync is not what you would use\nin a GUI application, instead you would want to request_async, then take action in a callback.\nNow, let's look at createLayers, which is where the magic happens.\nA timeline is a stack of layers, with ascending \"priorities\". Thanks to these layers, we are able for example\nto decide if a transition has to be created between two track objects, or, if two clips have an alpha of 1.0,\nwhich one will be the \"topmost\" one.\nThis code is very interesting. We are basically asking GES to : create a clip based on the asset we\ndiscovered earlier, set its start a i * 0.3 seconds, its inpoint (the place in the file from which it will\nbe played) to 0, and its duration to the original duration of the file.\nThe last argument means : for every kind of stream you find, add it if the timeline contains an\nappropriate track (here, audio and video).\nWe could have decided to only keep the VIDEO, but that was a good occasion to show that.\nWith that logic, we can now see that the resulting timeline is gonna be sort of a \"canon\":\none video mixed with n earlier versions of itself.\nHere, I browse the children of my timeline element, and when I find a video element, I set the\nalpha of an element inside it, and update the alpha. The log here makes it so each layer\nhas the same perceived opacity at the end.\nAfterwards, we create a pipeline to play our timeline, and if needed we set it to the render mode,\nthat code is quite self-explanatory.\nWe now just have to wait until the EOS, or until the user interrupts the program.\nI use Gtk.main() out of pure laziness, a GLib mainloop would work as well.\n"});